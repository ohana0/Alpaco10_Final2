{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting monai\n",
      "  Downloading monai-1.3.2-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: torch>=1.9 in /home/alpaco/anaconda3/envs/msk/lib/python3.8/site-packages (from monai) (1.13.1)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/alpaco/anaconda3/envs/msk/lib/python3.8/site-packages (from monai) (1.24.4)\n",
      "Requirement already satisfied: typing_extensions in /home/alpaco/anaconda3/envs/msk/lib/python3.8/site-packages (from torch>=1.9->monai) (4.12.2)\n",
      "Downloading monai-1.3.2-py3-none-any.whl (1.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: monai\n",
      "Successfully installed monai-1.3.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install monai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import monai\"\n",
    "!pip install -q \"monai-weekly[nibabel]\"\n",
    "!python -c \"import matplotlib\"\n",
    "!pip install -q matplotlib\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alpaco/anaconda3/envs/msk/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.3.2\n",
      "Numpy version: 1.24.4\n",
      "Pytorch version: 1.13.1\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: 59a7211070538586369afd4a01eca0a7fe2e742e\n",
      "MONAI __file__: /home/<username>/anaconda3/envs/msk/lib/python3.8/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "ITK version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.2.1\n",
      "scikit-image version: 0.21.0\n",
      "scipy version: 1.10.1\n",
      "Pillow version: 10.4.0\n",
      "Tensorboard version: 2.13.0\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.14.1\n",
      "tqdm version: 4.67.0\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 6.1.0\n",
      "pandas version: 2.0.3\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "clearml version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai import transforms\n",
    "from monai.transforms import (\n",
    "    AsDiscrete,\n",
    "    Activations,\n",
    ")\n",
    "\n",
    "from monai.config import print_config\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.utils.enums import MetricReduction\n",
    "from monai.networks.nets import SwinUNETR\n",
    "from monai import data\n",
    "from monai.data import decollate_batch\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/tmpvg7dn70r\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "if directory is not None:\n",
    "    os.makedirs(directory, exist_ok=True)\n",
    "root_dir = tempfile.mkdtemp() if directory is None else directory\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = np.where(self.count > 0, self.sum / self.count, self.sum)\n",
    "\n",
    "\n",
    "def datafold_read(datalist, basedir, fold=0, key=\"training\"):\n",
    "    with open(datalist) as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    json_data = json_data[key]\n",
    "\n",
    "    for d in json_data:\n",
    "        for k in d:\n",
    "            if isinstance(d[k], list):\n",
    "                d[k] = [os.path.join(basedir, iv) for iv in d[k]]\n",
    "            elif isinstance(d[k], str):\n",
    "                d[k] = os.path.join(basedir, d[k]) if len(d[k]) > 0 else d[k]\n",
    "\n",
    "    tr = []\n",
    "    val = []\n",
    "    for d in json_data:\n",
    "        if \"fold\" in d and d[\"fold\"] == fold:\n",
    "            val.append(d)\n",
    "        else:\n",
    "            tr.append(d)\n",
    "\n",
    "    return tr, val\n",
    "\n",
    "\n",
    "def save_checkpoint(model, epoch, filename=\"model.pt\", best_acc=0, dir_add=root_dir):\n",
    "    state_dict = model.state_dict()\n",
    "    save_dict = {\"epoch\": epoch, \"best_acc\": best_acc, \"state_dict\": state_dict}\n",
    "    filename = os.path.join(dir_add, filename)\n",
    "    torch.save(save_dict, filename)\n",
    "    print(\"Saving checkpoint\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader(batch_size, data_dir, json_list, fold, roi):\n",
    "    data_dir = data_dir\n",
    "    datalist_json = json_list\n",
    "    train_files, validation_files = datafold_read(datalist=datalist_json, basedir=data_dir, fold=fold)\n",
    "    train_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "            transforms.CropForegroundd(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                source_key=\"image\",\n",
    "                k_divisible=[roi[0], roi[1], roi[2]],\n",
    "            ),\n",
    "            transforms.RandSpatialCropd(\n",
    "                keys=[\"image\", \"label\"],\n",
    "                roi_size=[roi[0], roi[1], roi[2]],\n",
    "                random_size=False,\n",
    "            ),\n",
    "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "            transforms.RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "            transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "            transforms.RandScaleIntensityd(keys=\"image\", factors=0.1, prob=1.0),\n",
    "            transforms.RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=1.0),\n",
    "        ]\n",
    "    )\n",
    "    val_transform = transforms.Compose(\n",
    "        [\n",
    "            transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "            transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "            transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    train_ds = data.Dataset(data=train_files, transform=train_transform)\n",
    "\n",
    "    train_loader = data.DataLoader(\n",
    "        train_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    val_ds = data.Dataset(data=validation_files, transform=val_transform)\n",
    "    val_loader = data.DataLoader(\n",
    "        val_ds,\n",
    "        batch_size=1,\n",
    "        shuffle=False,\n",
    "        num_workers=8,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/data/brats2021challenge\"\n",
    "json_list = \"./brats21_folds.json\"\n",
    "roi = (128, 128, 128)\n",
    "batch_size = 2\n",
    "sw_batch_size = 4\n",
    "fold = 1\n",
    "infer_overlap = 0.5\n",
    "max_epochs = 100\n",
    "val_every = 10\n",
    "train_loader, val_loader = get_loader(batch_size, data_dir, json_list, fold, roi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_add = os.path.join(data_dir, \"TrainingData/BraTS2021_00006/BraTS2021_00006_flair.nii.gz\")\n",
    "label_add = os.path.join(data_dir, \"TrainingData/BraTS2021_00006/BraTS2021_00006_seg.nii.gz\")\n",
    "img = nib.load(img_add).get_fdata()\n",
    "label = nib.load(label_add).get_fdata()\n",
    "print(f\"image shape: {img.shape}, label shape: {label.shape}\")\n",
    "plt.figure(\"image\", (18, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(img[:, :, 78], cmap=\"gray\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[:, :, 78])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = SwinUNETR(\n",
    "    img_size=roi,\n",
    "    in_channels=4,\n",
    "    out_channels=3,\n",
    "    feature_size=48,\n",
    "    drop_rate=0.0,\n",
    "    attn_drop_rate=0.0,\n",
    "    dropout_path_rate=0.0,\n",
    "    use_checkpoint=True,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "dice_loss = DiceLoss(to_onehot_y=False, sigmoid=True)\n",
    "post_sigmoid = Activations(sigmoid=True)\n",
    "post_pred = AsDiscrete(argmax=False, threshold=0.5)\n",
    "dice_acc = DiceMetric(include_background=True, reduction=MetricReduction.MEAN_BATCH, get_not_nans=True)\n",
    "model_inferer = partial(\n",
    "    sliding_window_inference,\n",
    "    roi_size=[roi[0], roi[1], roi[2]],\n",
    "    sw_batch_size=sw_batch_size,\n",
    "    predictor=model,\n",
    "    overlap=infer_overlap,\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, epoch, loss_func):\n",
    "    model.train()\n",
    "    start_time = time.time()\n",
    "    run_loss = AverageMeter()\n",
    "    for idx, batch_data in enumerate(loader):\n",
    "        data, target = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "        logits = model(data)\n",
    "        loss = loss_func(logits, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        run_loss.update(loss.item(), n=batch_size)\n",
    "        print(\n",
    "            \"Epoch {}/{} {}/{}\".format(epoch, max_epochs, idx, len(loader)),\n",
    "            \"loss: {:.4f}\".format(run_loss.avg),\n",
    "            \"time {:.2f}s\".format(time.time() - start_time),\n",
    "        )\n",
    "        start_time = time.time()\n",
    "    return run_loss.avg\n",
    "\n",
    "\n",
    "def val_epoch(\n",
    "    model,\n",
    "    loader,\n",
    "    epoch,\n",
    "    acc_func,\n",
    "    model_inferer=None,\n",
    "    post_sigmoid=None,\n",
    "    post_pred=None,\n",
    "):\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    run_acc = AverageMeter()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, batch_data in enumerate(loader):\n",
    "            data, target = batch_data[\"image\"].to(device), batch_data[\"label\"].to(device)\n",
    "            logits = model_inferer(data)\n",
    "            val_labels_list = decollate_batch(target)\n",
    "            val_outputs_list = decollate_batch(logits)\n",
    "            val_output_convert = [post_pred(post_sigmoid(val_pred_tensor)) for val_pred_tensor in val_outputs_list]\n",
    "            acc_func.reset()\n",
    "            acc_func(y_pred=val_output_convert, y=val_labels_list)\n",
    "            acc, not_nans = acc_func.aggregate()\n",
    "            run_acc.update(acc.cpu().numpy(), n=not_nans.cpu().numpy())\n",
    "            dice_tc = run_acc.avg[0]\n",
    "            dice_wt = run_acc.avg[1]\n",
    "            dice_et = run_acc.avg[2]\n",
    "            print(\n",
    "                \"Val {}/{} {}/{}\".format(epoch, max_epochs, idx, len(loader)),\n",
    "                \", dice_tc:\",\n",
    "                dice_tc,\n",
    "                \", dice_wt:\",\n",
    "                dice_wt,\n",
    "                \", dice_et:\",\n",
    "                dice_et,\n",
    "                \", time {:.2f}s\".format(time.time() - start_time),\n",
    "            )\n",
    "            start_time = time.time()\n",
    "\n",
    "    return run_acc.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    loss_func,\n",
    "    acc_func,\n",
    "    scheduler,\n",
    "    model_inferer=None,\n",
    "    start_epoch=0,\n",
    "    post_sigmoid=None,\n",
    "    post_pred=None,\n",
    "):\n",
    "    val_acc_max = 0.0\n",
    "    dices_tc = []\n",
    "    dices_wt = []\n",
    "    dices_et = []\n",
    "    dices_avg = []\n",
    "    loss_epochs = []\n",
    "    trains_epoch = []\n",
    "    for epoch in range(start_epoch, max_epochs):\n",
    "        print(time.ctime(), \"Epoch:\", epoch)\n",
    "        epoch_time = time.time()\n",
    "        train_loss = train_epoch(\n",
    "            model,\n",
    "            train_loader,\n",
    "            optimizer,\n",
    "            epoch=epoch,\n",
    "            loss_func=loss_func,\n",
    "        )\n",
    "        print(\n",
    "            \"Final training  {}/{}\".format(epoch, max_epochs - 1),\n",
    "            \"loss: {:.4f}\".format(train_loss),\n",
    "            \"time {:.2f}s\".format(time.time() - epoch_time),\n",
    "        )\n",
    "\n",
    "        if (epoch + 1) % val_every == 0 or epoch == 0:\n",
    "            loss_epochs.append(train_loss)\n",
    "            trains_epoch.append(int(epoch))\n",
    "            epoch_time = time.time()\n",
    "            val_acc = val_epoch(\n",
    "                model,\n",
    "                val_loader,\n",
    "                epoch=epoch,\n",
    "                acc_func=acc_func,\n",
    "                model_inferer=model_inferer,\n",
    "                post_sigmoid=post_sigmoid,\n",
    "                post_pred=post_pred,\n",
    "            )\n",
    "            dice_tc = val_acc[0]\n",
    "            dice_wt = val_acc[1]\n",
    "            dice_et = val_acc[2]\n",
    "            val_avg_acc = np.mean(val_acc)\n",
    "            print(\n",
    "                \"Final validation stats {}/{}\".format(epoch, max_epochs - 1),\n",
    "                \", dice_tc:\",\n",
    "                dice_tc,\n",
    "                \", dice_wt:\",\n",
    "                dice_wt,\n",
    "                \", dice_et:\",\n",
    "                dice_et,\n",
    "                \", Dice_Avg:\",\n",
    "                val_avg_acc,\n",
    "                \", time {:.2f}s\".format(time.time() - epoch_time),\n",
    "            )\n",
    "            dices_tc.append(dice_tc)\n",
    "            dices_wt.append(dice_wt)\n",
    "            dices_et.append(dice_et)\n",
    "            dices_avg.append(val_avg_acc)\n",
    "            if val_avg_acc > val_acc_max:\n",
    "                print(\"new best ({:.6f} --> {:.6f}). \".format(val_acc_max, val_avg_acc))\n",
    "                val_acc_max = val_avg_acc\n",
    "                save_checkpoint(\n",
    "                    model,\n",
    "                    epoch,\n",
    "                    best_acc=val_acc_max,\n",
    "                )\n",
    "            scheduler.step()\n",
    "    print(\"Training Finished !, Best Accuracy: \", val_acc_max)\n",
    "    return (\n",
    "        val_acc_max,\n",
    "        dices_tc,\n",
    "        dices_wt,\n",
    "        dices_et,\n",
    "        dices_avg,\n",
    "        loss_epochs,\n",
    "        trains_epoch,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "\n",
    "(\n",
    "    val_acc_max,\n",
    "    dices_tc,\n",
    "    dices_wt,\n",
    "    dices_et,\n",
    "    dices_avg,\n",
    "    loss_epochs,\n",
    "    trains_epoch,\n",
    ") = trainer(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    optimizer=optimizer,\n",
    "    loss_func=dice_loss,\n",
    "    acc_func=dice_acc,\n",
    "    scheduler=scheduler,\n",
    "    model_inferer=model_inferer,\n",
    "    start_epoch=start_epoch,\n",
    "    post_sigmoid=post_sigmoid,\n",
    "    post_pred=post_pred,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"train completed, best average dice: {val_acc_max:.4f} \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(trains_epoch, loss_epochs, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(trains_epoch, dices_avg, color=\"green\")\n",
    "plt.show()\n",
    "plt.figure(\"train\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Val Mean Dice TC\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(trains_epoch, dices_tc, color=\"blue\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice WT\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(trains_epoch, dices_wt, color=\"brown\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice ET\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(trains_epoch, dices_et, color=\"purple\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case_num = \"01619\"\n",
    "\n",
    "test_files = [\n",
    "    {\n",
    "        \"image\": [\n",
    "            os.path.join(\n",
    "                data_dir,\n",
    "                \"TrainingData/BraTS2021_\" + case_num + \"/BraTS2021_\" + case_num + \"_flair.nii.gz\",\n",
    "            ),\n",
    "            os.path.join(\n",
    "                data_dir,\n",
    "                \"TrainingData/BraTS2021_\" + case_num + \"/BraTS2021_\" + case_num + \"_t1ce.nii.gz\",\n",
    "            ),\n",
    "            os.path.join(\n",
    "                data_dir,\n",
    "                \"TrainingData/BraTS2021_\" + case_num + \"/BraTS2021_\" + case_num + \"_t1.nii.gz\",\n",
    "            ),\n",
    "            os.path.join(\n",
    "                data_dir,\n",
    "                \"TrainingData/BraTS2021_\" + case_num + \"/BraTS2021_\" + case_num + \"_t2.nii.gz\",\n",
    "            ),\n",
    "        ],\n",
    "        \"label\": os.path.join(\n",
    "            data_dir,\n",
    "            \"TrainingData/BraTS2021_\" + case_num + \"/BraTS2021_\" + case_num + \"_seg.nii.gz\",\n",
    "        ),\n",
    "    }\n",
    "]\n",
    "\n",
    "test_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        transforms.ConvertToMultiChannelBasedOnBratsClassesd(keys=\"label\"),\n",
    "        transforms.NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "test_ds = data.Dataset(data=test_files, transform=test_transform)\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=8,\n",
    "    pin_memory=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(os.path.join(root_dir, \"model.pt\"))[\"state_dict\"])\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "model_inferer_test = partial(\n",
    "    sliding_window_inference,\n",
    "    roi_size=[roi[0], roi[1], roi[2]],\n",
    "    sw_batch_size=1,\n",
    "    predictor=model,\n",
    "    overlap=0.6,\n",
    ")\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_data in test_loader:\n",
    "        image = batch_data[\"image\"].cuda()\n",
    "        prob = torch.sigmoid(model_inferer_test(image))\n",
    "        seg = prob[0].detach().cpu().numpy()\n",
    "        seg = (seg > 0.5).astype(np.int8)\n",
    "        seg_out = np.zeros((seg.shape[1], seg.shape[2], seg.shape[3]))\n",
    "        seg_out[seg[1] == 1] = 2\n",
    "        seg_out[seg[0] == 1] = 1\n",
    "        seg_out[seg[2] == 1] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slice_num = 67\n",
    "img_add = os.path.join(\n",
    "    data_dir,\n",
    "    \"TrainingData/BraTS2021_\" + case_num + \"/BraTS2021_\" + case_num + \"_t1ce.nii.gz\",\n",
    ")\n",
    "label_add = os.path.join(\n",
    "    data_dir,\n",
    "    \"TrainingData/BraTS2021_\" + case_num + \"/BraTS2021_\" + case_num + \"_seg.nii.gz\",\n",
    ")\n",
    "img = nib.load(img_add).get_fdata()\n",
    "label = nib.load(label_add).get_fdata()\n",
    "plt.figure(\"image\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"image\")\n",
    "plt.imshow(img[:, :, slice_num], cmap=\"gray\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"label\")\n",
    "plt.imshow(label[:, :, slice_num])\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"segmentation\")\n",
    "plt.imshow(seg_out[:, :, slice_num])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "msk",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
